{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab: Unstructured Data**\n",
    "\n",
    "## Exercise 3: Text Classification\n",
    "\n",
    "In this exercise, we will train a Pytorch model with embeddings for classifying texts into topics. We will be working on the DBpedia dataset:\n",
    "https://wiki.dbpedia.org/about\n",
    "\n",
    "The steps are:\n",
    "1.   Setup Repository\n",
    "2.   Load Dataset\n",
    "3.   Prepare Data\n",
    "4.   Define Architecture\n",
    "5.   Train Model\n",
    "6.   Push Changes\n",
    "\n",
    "**Note**: this lab initially used dataset DBpedia. But that and some others would not work. <br>AGNews worked fine - it doesn't have any access/download issues (yet).<br> \n",
    "https://pytorch.org/text/0.8.1/_modules/torchtext/datasets/text_classification.html\n",
    "\n",
    "### 1. Setup Repository\n",
    "\n",
    "**[1.1]** Go inside the created folder `adv_dsi_lab_6`\n",
    "\n",
    "**[1.2]** Create a new git branch called pytorch_dbpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go inside the created folder adv_dsi_lab_6\n",
    "cd adv_dsi_lab_6\n",
    "\n",
    "# Create a new git branch called pytorch_dbpedia\n",
    "git checkout -b pytorch_dbpedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[1.3]** Run the built image\n",
    "\n",
    "**[1.4]** Display last 50 lines of logs\n",
    "- Copy the url displayed and paste it to a browser in order to launch Jupyter Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the built image\n",
    "docker run  -dit --rm --name adv_dsi_lab_6 -p 8888:8888 -e JUPYTER_ENABLE_LAB=yes -v ~/Projects/adv_dsi/adv_dsi_lab_6:/home/jovyan/work -v ~/Projects/adv_dsi/src:/home/jovyan/work/src -v ~/Projects/adv_dsi/data:/home/jovyan/work/data pytorch-notebook:latest \n",
    "                    \n",
    "# Display last 50 lines of logs\n",
    "docker logs --tail 50 adv_dsi_lab_6             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[1.4]** Launch the magic commands for auto-relaoding external modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the magic commands for auto-relaoding external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.   Load Dataset\n",
    "\n",
    "**[2.0]** Run !pip install once only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.7.1+cpu in /opt/conda/lib/python3.7/site-packages (1.7.1+cpu)\n",
      "Requirement already satisfied: torchvision==0.8.2+cpu in /opt/conda/lib/python3.7/site-packages (0.8.2+cpu)\n",
      "Requirement already satisfied: torchtext==0.8.1 in /opt/conda/lib/python3.7/site-packages (0.8.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.7.1+cpu) (4.1.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==1.7.1+cpu) (1.18.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.8.2+cpu) (7.0.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext==0.8.1) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext==0.8.1) (4.43.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.8.1) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.8.1) (1.25.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.8.1) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.8.1) (2.9)\n"
     ]
    }
   ],
   "source": [
    "# Run !pip install once only\n",
    "!pip install torch==1.7.1+cpu torchvision==0.8.2+cpu torchtext==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "    \n",
    "# Attempted newer versions but they have not fixed the bug\n",
    "#!pip install torch==1.8.1+cpu torchvision==0.9.1+cpu torchtext==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2.1]** Import torch, torchtext and text_classification from torchtext.datasets\n",
    "\n",
    "**[2.2]** Create 2 variables called `NGRAMS` and `BATCH_SIZE` that will contain respectively the values 2 and 16\n",
    "\n",
    "**[2.3]** Dowload DBpedia dataset into `data/raw/` folder with 2 ngrams and no vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.path.isdir(\"./.data\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.path.isdir('./data/raw'):\n",
    "    # os.mkdir('./.data')\n",
    "    print('os.path.isdir(\"./.data\")')\n",
    "\n",
    "# playing with versions\n",
    "import pkg_resources\n",
    "# pkg_resources.require(\"torchtext==0.8.0\")\n",
    "# import torchtext    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torchtext\n",
      "Version: 0.9.0\n",
      "Summary: Text utilities and datasets for PyTorch\n",
      "Home-page: https://github.com/pytorch/text\n",
      "Author: PyTorch core devs and James Bradbury\n",
      "Author-email: jekbradbury@gmail.com\n",
      "License: BSD\n",
      "Location: /opt/conda/lib/python3.7/site-packages\n",
      "Requires: numpy, torch, requests, tqdm\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torchtext.datasets import text_classification\n",
    "- version 0.8.1 https://pytorch.org/text/0.8.1/_modules/torchtext/datasets/text_classification.html\n",
    "- Think there is an issue with the datasets - only AG_NEWS works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:10, 11317.07lines/s]\n",
      "120000lines [00:21, 5545.51lines/s]\n",
      "7600lines [00:01, 5144.64lines/s]\n"
     ]
    }
   ],
   "source": [
    "# Import torch and torchtext\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import text_classification\n",
    "\n",
    "# Create 2 variables called NGRAMS and BATCH_SIZE that will contain respectively the values 2 and 16\n",
    "NGRAMS = 2\n",
    "BATCH_SIZE = 16\n",
    "UKNOWN_TOK=True\n",
    "\n",
    "# Dowload DBpedia dataset into data/raw/ folder with 2 ngrams and no vocabulary\n",
    "# train_dataset, test_dataset = text_classification.DATASETS['DBpedia'](root='../data/raw', ngrams=NGRAMS, vocab=None)\n",
    "# train_dataset, test_dataset = text_classification.DATASETS['DBpedia'](root='./data/raw', ngrams=NGRAMS, vocab=None, include_unk=UKNOWN_TOK)\n",
    "\n",
    "# Think access issue with DBpedia, use AG_NEWS instead - works fine\n",
    "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](root='./data', ngrams=NGRAMS, vocab=None), include_unk=UKNOWN_TOK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2.4]** Print the length of the training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000\n",
      "7600\n"
     ]
    }
   ],
   "source": [
    "# Print the length of the training and testing sets\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare Data\n",
    "\n",
    "**[3.1]** Extract the size of the vocabulary from the training set and save it into a variable called `VOCAB_SIZE`\n",
    "\n",
    "**[3.2]** Extract the number of classes from the training set and save it into a variable called `NUM_CLASS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the size of the vocabulary from the training set and save it into a variable called VOCAB_SIZE\n",
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "\n",
    "# Extract the number of classes from the training set and save it into a variable called NUM_CLASS\n",
    "NUM_CLASS = len(train_dataset.get_labels())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3.3]** Import random_split from torch.utils.data.dataset\n",
    "\n",
    "**[3.4]** Create 2 variables called `train_len` and `valid_len` that will  contain values that represent respectively 95% and 5% of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import random_split from torch.utils.data.dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "# Create 2 variables called train_len and valid_len that will contain values that represent respectively 95% and 5% of the training data\n",
    "train_len = int(len(train_dataset) * 0.95)\n",
    "valid_len = len(train_dataset) - train_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3.5]** Split the training data into training and validation sets\n",
    "\n",
    "**[3.6]** Create a generator on `train_data` and extract the first observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    101,    7852,    9472,     559,       7,    3175,      12,       3,\n",
       "            528,   12337,     139,       7,       3,     516,      99,     320,\n",
       "            511,   24192,     158,    6900,       4,    1360,    3175,      39,\n",
       "           1002,      47,      22,    1649,     709,       6,     586,    2419,\n",
       "              2,   59907, 1079516,  663215,    3559,  158314,  103366,      60,\n",
       "          48076,  975461,   66409,    1705,      29,    1958, 1250075,   54120,\n",
       "         281037, 1261818,  109992,   21110,   40499,  299012,    8923,  163675,\n",
       "           7341,    9746,   13580,   48861,  830861,    5872,    3432,  926778,\n",
       "          52114])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the training data into training and validation sets\n",
    "train_data, valid_data = random_split(train_dataset, [train_len, valid_len])\n",
    "\n",
    "# Create a generator on train_data and extract the first observation\n",
    "examples = enumerate(train_data)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "example_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3.7]** Define a function that will extract label and target from a batch of observation, calculate the length of each text and store them as offset (highlight start of new text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dimensions of the first image\n",
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1] for entry in batch]\n",
    "    \n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define Architecture\n",
    "\n",
    "**[4.1]** Create a class called `TextTopic` that inherits from `nn.Module` with:\n",
    "- inputs:\n",
    "    - vocabulary size\n",
    "    - embedding dimension\n",
    "    - number of classes\n",
    "- attributes:\n",
    "    - `embedding`: bag of embedding of shape: vocabulary size * embedding dimension \n",
    "    - `fc`: fully-connected layer with the number of neurons equals to the number of classes\n",
    "    - `softmax`: Softmax activation function\n",
    "- methods:\n",
    "    - `forward()` with `text` and `offsets` as input parameters and will sequentially add the embedding layer with dropout followed by the fully connected layer with dropout and softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4.2]** Import torch.nn as nn and torch.nn.functional as F\n",
    "\n",
    "**[4.3]** Create a variable called `EMBED_DIM` that will contain the value 32\n",
    "\n",
    "**[4.4]** Instantiate a `TextTopic()` and save it into a variable called `model` \n",
    "\n",
    "**[4.5]** Import the `get_device` function from src.models.pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torch.nn as nn and torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create a variable called EMBED_DIM that will contain the value 32\n",
    "EMBED_DIM = 32\n",
    "\n",
    "# Import from `pytorch.py` & instantiate a TextTopic() and save it into a variable called model\n",
    "from src.models.pytorch import TextTopic\n",
    "model = TextTopic(VOCAB_SIZE, EMBED_DIM, NUM_CLASS)\n",
    "\n",
    "# Import the get_device function from src.models.pytorch\n",
    "#from src.models.pytorch import get_device\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu') # don't have GPU \n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4.6]** Get the device available and set to the model to use it\n",
    "\n",
    "**[4.7]** Print the architecture of this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextTopic(\n",
       "  (embedding): EmbeddingBag(1308844, 32, mode=mean)\n",
       "  (fc): Linear(in_features=32, out_features=4, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the device available and set to the model to use it\n",
    "device = get_device()\n",
    "model.to(device)\n",
    "\n",
    "# Print the architecture of this model\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train the model\n",
    "\n",
    "**[5.1]** Import DataLoader from torch.utils.data\n",
    "\n",
    "**[5.2]** Create a variable called `N_EPOCHS` that will take the value 5\n",
    "\n",
    "**[5.3]** Instantiate a `nn.CrossEntropyLoss()` and save it into a variable called `criterion`\n",
    "\n",
    "**[5.6]** Instantiate a `torch.optim.SGD()` optimizer with the model's parameters and 4 as learning rate and save it into a variable called `optimizer`\n",
    "\n",
    "**[5.7]** Instantiate a `torch.optim.lr_scheduler.StepLR()` scheduler that will decrease the learning rate by a coefficient of 0.9 for each epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_binary and test_binary from src.models.pytorch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a variable called N_EPOCHS that will take the value 5\n",
    "N_EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Import train_classification and test_classification from src.models.pytorch\n",
    "from src.models.pytorch import train_classification_unstruct, test_classification_unstruct\n",
    "\n",
    "# Instantiate a nn.CrossEntropyLoss() and save it into a variable called criterion\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Instantiate a torch.optim.SGD() optimizer with the model's parameters and 4 as learning rate \n",
    "# and save it into a variable called optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "\n",
    "# Instantiate a torch.optim.lr_scheduler.StepLR() scheduler that will \n",
    "# decrease the learning rate by a coefficient of 0.9 for each epoch\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5.8]** Create a for loop that will iterate through the specified number of epochs and will train the model with the training set and assess the performance on the validation set and print their scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "\t(train)\t|\tLoss: 0.0382\t|\tAcc: 49.7%\n",
      "\t(valid)\t|\tLoss: 0.0349\t|\tAcc: 62.0%\n",
      "Epoch: 1\n",
      "\t(train)\t|\tLoss: 0.0335\t|\tAcc: 65.8%\n",
      "\t(valid)\t|\tLoss: 0.0329\t|\tAcc: 67.6%\n",
      "Epoch: 2\n",
      "\t(train)\t|\tLoss: 0.0321\t|\tAcc: 70.0%\n",
      "\t(valid)\t|\tLoss: 0.0321\t|\tAcc: 70.0%\n",
      "Epoch: 3\n",
      "\t(train)\t|\tLoss: 0.0315\t|\tAcc: 72.2%\n",
      "\t(valid)\t|\tLoss: 0.0316\t|\tAcc: 71.8%\n",
      "Epoch: 4\n",
      "\t(train)\t|\tLoss: 0.0310\t|\tAcc: 73.6%\n",
      "\t(valid)\t|\tLoss: 0.0313\t|\tAcc: 72.8%\n"
     ]
    }
   ],
   "source": [
    "# Create a for loop that will iterate through the specified number of epochs and will train the model with the training set and \n",
    "# assess the performance on the validation set and print their scores\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train_classification_unstruct(train_data, model, criterion, optimizer, batch_size=BATCH_SIZE, device=device, scheduler=scheduler, generate_batch=generate_batch)\n",
    "    valid_loss, valid_acc = test_classification_unstruct(valid_data, model, criterion, batch_size=BATCH_SIZE, device=device, generate_batch=generate_batch)\n",
    "\n",
    "    print(f'Epoch: {epoch}')\n",
    "    print(f'\\t(train)\\t|\\tLoss: {train_loss:.4f}\\t|\\tAcc: {train_acc * 100:.1f}%')\n",
    "    print(f'\\t(valid)\\t|\\tLoss: {valid_loss:.4f}\\t|\\tAcc: {valid_acc * 100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5.6]** Save the model into the `models` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model into the models folder\n",
    "torch.save(model, \"../models/pytorch_agnews.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.   Push changes\n",
    "\n",
    "**[6.1]** Add you changes to git staging area\n",
    "\n",
    "**[6.2]** Create the snapshot of your repository and add a description\n",
    "\n",
    "**[6.3]** Push your snapshot to Github\n",
    "\n",
    "**[6.4]** Check out to the master branch\n",
    "\n",
    "**[6.5]** Pull the latest updates\n",
    "\n",
    "**[6.6]** Check out to the `pytorch_mnist` branch\n",
    "\n",
    "**[6.7]** Merge the `master` branch and push your changes\n",
    "\n",
    "**[6.8]** Go to Github and merge the branch after reviewing the code and fixing any conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Add you changes to git staging area\n",
    "git add .\n",
    "\n",
    "# Create the snapshot of your repository and add a description\n",
    "git commit -m \"pytorch dbpedia\"\n",
    "\n",
    "# Push your snapshot to Github\n",
    "git push https://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@github.com/CazMayhem/adv_dsi_lab_6.git \n",
    "\n",
    "# Check out to the master branch\n",
    "git checkout master\n",
    "\n",
    "# Pull the latest updates\n",
    "git pull https://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@github.com/CazMayhem/adv_dsi_lab_6.git \n",
    "\n",
    "# Check out to the pytorch_mnist branch\n",
    "git checkout pytorch_dbpedia\n",
    "\n",
    "# erge the master branch and push your changes\n",
    "git merge master\n",
    "git push https://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@github.com/CazMayhem/adv_dsi_lab_6.git \n",
    "\n",
    "# Now go to Github and merge the branch after reviewing the code and fixing any conflict\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[6.9]** Stop the Docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Docker container\n",
    "docker stop adv_dsi_lab_6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
